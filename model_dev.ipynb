{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from preprocessing import tokenize, embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/df_lyrics.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in set(df.Genre):\n",
    "    print(genre, df[df.Genre == genre].shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenize.Tokenizer()\n",
    "tokenizer.load(path = \"Weights/tokenizer.json\")\n",
    "tokenizer.tokenize(\"I'm a little teapot\", get_token_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "MAX_LENGTH = 1024  # max context length for the tokenizer\n",
    "##########################################################\n",
    "lyrics = [l[:min(len(l), MAX_LENGTH)] for l in list(df[\"Lyrics\"])]\n",
    "\n",
    "proprocessed_lyrics = tokenizer.tokenize(lyrics, get_token_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_token_ids = proprocessed_lyrics[\"token_ids\"]\n",
    "len(lyrics_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(len(lyrics_token_ids))\n",
    "\n",
    "####################################################################\n",
    "training_size = 45000\n",
    "validation_size = 15000\n",
    "test_size = len(lyrics_token_ids) - training_size - validation_size\n",
    "####################################################################\n",
    "\n",
    "training_data = [lyrics_token_ids[i] for i in shuffle_idx[:training_size]]\n",
    "validation_data = [lyrics_token_ids[i] for i in shuffle_idx[training_size : training_size + validation_size]]\n",
    "test_data = [lyrics_token_ids[i] for i in shuffle_idx[training_size + validation_size : ]]\n",
    "\n",
    "print(len(training_data), len(validation_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = embeddings.Embedding()\n",
    "embedder.load(\"Weights/embeddings_w2v.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.embeddings_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros(embedder.vocab_size, embedder.embeddings_size)\n",
    "for idx in tqdm(tokenizer.index_word):\n",
    "    word = tokenizer.index_word[idx]\n",
    "    vec = embedder.model.get_vector(word)\n",
    "    print(idx, vec)\n",
    "    embedding_matrix[idx] = torch.tensor(vec)\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers = 8):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim = embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size = embedding_dim, \n",
    "            hidden_size = hidden_dim,\n",
    "            num_layers = num_layers,\n",
    "        )\n",
    "        self.decoder = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.word_embeddings(input)\n",
    "        output, hidden = self.rnn(embeds, hidden)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(\n",
    "    embedding_dim = embedder.embeddings_size,\n",
    "    hidden_dim = embedder.embeddings_size,\n",
    "    vocab_size = embedder.vocab_size\n",
    ")\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedder = rnn.word_embeddings\n",
    "for p in model_embedder.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"hello\"\n",
    "embed_vector = embedder.model.get_vector(word)\n",
    "tokenizer.keras_tokenizer.word_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedder(torch.tensor(tokenizer.keras_tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
